-- ====================================
-- TRACKANYTHING: SCRAPING JOBS & SITES TABLES
-- ====================================
-- Run this script in your Supabase SQL Editor to add scraping functionality
-- This adds scraping_jobs, scrapable_sites, and scraping_job_sites tables

-- ====================================
-- SCRAPING JOBS TABLE
-- ====================================

CREATE TABLE scraping_jobs (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    profile_id UUID NOT NULL REFERENCES profiles(id) ON DELETE CASCADE,
    topic_id BIGINT NOT NULL REFERENCES topics(id) ON DELETE CASCADE,
    
    -- Job configuration
    name TEXT NOT NULL,
    description TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    
    -- Data sources configuration (JSON array of enabled API sources)
    enabled_sources JSONB DEFAULT '["gnews", "serpapi", "dr"]'::jsonb,
    
    -- Scraping configuration
    max_articles_per_source INTEGER DEFAULT 50,
    hours_lookback INTEGER DEFAULT 24,
    
    -- Status and tracking
    last_run_at TIMESTAMPTZ,
    total_mentions_found INTEGER DEFAULT 0,
    last_error_message TEXT,
    
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ====================================
-- SCRAPABLE SITES TABLE
-- ====================================

CREATE TABLE scrapable_sites (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    
    -- Site information
    name TEXT NOT NULL,
    description TEXT,
    base_url TEXT NOT NULL,
    urls_to_scrape TEXT[] NOT NULL DEFAULT '{}',
    
    -- Scraping configuration
    article_selector TEXT DEFAULT "a[href]",
    exclude_patterns TEXT[] DEFAULT '{}',
    include_patterns TEXT[] DEFAULT '{}',
    
    -- Categorization and metadata
    category TEXT, -- 'news', 'tech', 'business', 'finance'
    language TEXT DEFAULT 'da',
    
    -- Performance stats (updated by scraping jobs)
    avg_articles_per_day INTEGER DEFAULT 0,
    success_rate DECIMAL(5,2) DEFAULT 0.00,
    last_scraped_at TIMESTAMPTZ,
    total_scrapes INTEGER DEFAULT 0,
    
    -- Status
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ====================================
-- JUNCTION TABLE: JOB <-> SITES
-- ====================================

CREATE TABLE scraping_job_sites (
    scraping_job_id BIGINT NOT NULL REFERENCES scraping_jobs(id) ON DELETE CASCADE,
    scrapable_site_id BIGINT NOT NULL REFERENCES scrapable_sites(id) ON DELETE CASCADE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    PRIMARY KEY (scraping_job_id, scrapable_site_id)
);

-- ====================================
-- INDEXES FOR PERFORMANCE
-- ====================================

-- scraping_jobs indexes
CREATE INDEX idx_scraping_jobs_profile_id ON scraping_jobs(profile_id);
CREATE INDEX idx_scraping_jobs_topic_id ON scraping_jobs(topic_id);
CREATE INDEX idx_scraping_jobs_is_active ON scraping_jobs(is_active);
CREATE INDEX idx_scraping_jobs_last_run_at ON scraping_jobs(last_run_at);

-- scrapable_sites indexes  
CREATE INDEX idx_scrapable_sites_category ON scrapable_sites(category);
CREATE INDEX idx_scrapable_sites_is_active ON scrapable_sites(is_active);
CREATE INDEX idx_scrapable_sites_language ON scrapable_sites(language);

-- junction table indexes
CREATE INDEX idx_scraping_job_sites_job_id ON scraping_job_sites(scraping_job_id);
CREATE INDEX idx_scraping_job_sites_site_id ON scraping_job_sites(scrapable_site_id);
CREATE INDEX idx_scraping_job_sites_active ON scraping_job_sites(is_active);

-- ====================================
-- UPDATED_AT TRIGGERS
-- ====================================

-- Function for updating updated_at (if not exists)
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Triggers for updated_at
CREATE TRIGGER update_scraping_jobs_updated_at 
    BEFORE UPDATE ON scraping_jobs 
    FOR EACH ROW 
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_scrapable_sites_updated_at 
    BEFORE UPDATE ON scrapable_sites 
    FOR EACH ROW 
    EXECUTE FUNCTION update_updated_at_column();

-- ====================================
-- ROW LEVEL SECURITY (RLS)
-- ====================================

-- Enable RLS on scraping_jobs
ALTER TABLE scraping_jobs ENABLE ROW LEVEL SECURITY;

-- RLS policies for scraping_jobs (users can only access their own jobs)
CREATE POLICY "Users can view their own scraping jobs" 
    ON scraping_jobs FOR SELECT 
    USING (profile_id = auth.uid());

CREATE POLICY "Users can create their own scraping jobs" 
    ON scraping_jobs FOR INSERT 
    WITH CHECK (profile_id = auth.uid());

CREATE POLICY "Users can update their own scraping jobs" 
    ON scraping_jobs FOR UPDATE 
    USING (profile_id = auth.uid());

CREATE POLICY "Users can delete their own scraping jobs" 
    ON scraping_jobs FOR DELETE 
    USING (profile_id = auth.uid());

-- scrapable_sites is globally readable (no RLS needed - managed by admins)
-- But we enable RLS for future admin-only write policies
ALTER TABLE scrapable_sites ENABLE ROW LEVEL SECURITY;

-- All users can read scrapable sites
CREATE POLICY "Anyone can view scrapable sites" 
    ON scrapable_sites FOR SELECT 
    USING (is_active = true);

-- Only admins can modify scrapable sites (for now, allow all - you can restrict later)
CREATE POLICY "Admins can manage scrapable sites" 
    ON scrapable_sites FOR ALL
    USING (true);

-- Junction table: Users can only manage their own job-site relationships
ALTER TABLE scraping_job_sites ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their own job-site relationships" 
    ON scraping_job_sites FOR SELECT 
    USING (
        EXISTS (
            SELECT 1 FROM scraping_jobs 
            WHERE scraping_jobs.id = scraping_job_sites.scraping_job_id 
            AND scraping_jobs.profile_id = auth.uid()
        )
    );

CREATE POLICY "Users can create their own job-site relationships" 
    ON scraping_job_sites FOR INSERT 
    WITH CHECK (
        EXISTS (
            SELECT 1 FROM scraping_jobs 
            WHERE scraping_jobs.id = scraping_job_sites.scraping_job_id 
            AND scraping_jobs.profile_id = auth.uid()
        )
    );

CREATE POLICY "Users can update their own job-site relationships" 
    ON scraping_job_sites FOR UPDATE 
    USING (
        EXISTS (
            SELECT 1 FROM scraping_jobs 
            WHERE scraping_jobs.id = scraping_job_sites.scraping_job_id 
            AND scraping_jobs.profile_id = auth.uid()
        )
    );

CREATE POLICY "Users can delete their own job-site relationships" 
    ON scraping_job_sites FOR DELETE 
    USING (
        EXISTS (
            SELECT 1 FROM scraping_jobs 
            WHERE scraping_jobs.id = scraping_job_sites.scraping_job_id 
            AND scraping_jobs.profile_id = auth.uid()
        )
    );

-- ====================================
-- SAMPLE DATA - Migrate Politiken + Add More Sites
-- ====================================

INSERT INTO scrapable_sites (name, description, base_url, urls_to_scrape, article_selector, exclude_patterns, category, avg_articles_per_day) VALUES 

('Politiken', 'Danmarks stÃ¸rste dagblad med bred nyhedsdÃ¦kning', 'https://politiken.dk', 
 ARRAY['/', '/senestenyt', '/danmark', '/udland', '/kultur'], 
 'a[href]', ARRAY['/premium/', '/abonnement/', '/paywall/'], 'news', 45),

('Version2', 'Danmarks fÃ¸rende tech-medie med fokus pÃ¥ IT og digitalisering', 'https://version2.dk',
 ARRAY['/artikel/', '/nyheder/', '/'], 
 'a[href]', ARRAY['/job/', '/events/', '/webinar/'], 'tech', 12),

('Computer.dk', 'IT-nyheder og teknologi for danske virksomheder', 'https://computer.dk',
 ARRAY['/nyheder/', '/artikler/', '/'], 
 'a[href]', ARRAY['/job/', '/produkter/'], 'tech', 8),

('Finans.dk', 'Business nyheder med tech og innovation focus', 'https://finans.dk', 
 ARRAY['/nyheder/', '/it/', '/tech/', '/'], 
 'a[href]', ARRAY['/abonnement/', '/job/', '/kurser/'], 'business', 20),

('BÃ¸rsen', 'Erhvervsnyheder og teknologi for business beslutningstagere', 'https://borsen.dk',
 ARRAY['/nyheder/', '/teknologi/', '/innovation/'], 
 'a[href]', ARRAY['/abonnement/', '/job/'], 'business', 35);

-- ====================================
-- HELPFUL VIEWS
-- ====================================

-- View for frontend: Sites with usage stats
CREATE VIEW scrapable_sites_with_stats AS 
SELECT 
    s.*,
    COUNT(sjs.scraping_job_id) as job_usage_count,
    COUNT(CASE WHEN sjs.is_active = true THEN 1 END) as active_job_count
FROM scrapable_sites s
LEFT JOIN scraping_job_sites sjs ON s.id = sjs.scrapable_site_id
WHERE s.is_active = true
GROUP BY s.id
ORDER BY job_usage_count DESC, s.avg_articles_per_day DESC;

-- View for job details with associated sites
CREATE VIEW scraping_jobs_with_sites AS
SELECT 
    sj.*,
    t.name as topic_name,
    b.name as brand_name,
    b.id as brand_id,
    ARRAY_AGG(
        CASE WHEN sjs.is_active = true 
        THEN json_build_object('id', ss.id, 'name', ss.name, 'category', ss.category)
        END
    ) FILTER (WHERE sjs.is_active = true) as active_sites
FROM scraping_jobs sj
JOIN topics t ON sj.topic_id = t.id
JOIN brands b ON t.brand_id = b.id
LEFT JOIN scraping_job_sites sjs ON sj.id = sjs.scraping_job_id AND sjs.is_active = true
LEFT JOIN scrapable_sites ss ON sjs.scrapable_site_id = ss.id
GROUP BY sj.id, t.name, b.name, b.id;

-- ====================================
-- SUCCESS! TABLES CREATED
-- ====================================
/*
ðŸŽ‰ SUCCESS! The following has been created:

TABLES:
âœ… scraping_jobs - User-owned scraping configurations
âœ… scrapable_sites - Platform-managed website sources  
âœ… scraping_job_sites - Many-to-many job<->site relationships

SAMPLE DATA:
âœ… 5 Danish sites added: Politiken, Version2, Computer.dk, Finans.dk, BÃ¸rsen

SECURITY:
âœ… Row Level Security (RLS) policies
âœ… Users can only access their own scraping jobs
âœ… All users can browse available scrapable sites

PERFORMANCE:
âœ… Strategic indexes on key lookup fields
âœ… Automatic updated_at triggers
âœ… Helpful views for frontend integration

NEXT STEPS:
1. Verify tables were created successfully in Supabase dashboard
2. Implement API endpoints for scraping jobs management
3. Update scraping_service.py to use job-specific site selections
4. Build frontend interface for job creation and site selection

Happy scraping! ðŸš€
*/