import json
import re
import httpx
from typing import Dict, Optional
from bs4 import BeautifulSoup
from openai import AsyncOpenAI
from app.core.config import settings
from app.core.selectors import GENERIC_SELECTORS_MAP

class AIAnalyzer:
    """
    Handles AI-powered analysis of HTML content to find selectors and verify quality.
    """

    async def verify_content_quality(self, text: str, type: str) -> bool:
        """
        Verify extracted text using AI (Judge).
        
        Args:
            text: The extracted text content
            type: 'title_selector' or 'content_selector'
        """
        if not text:
            return False

        # --- 1. Basic Heuristics (Fail Fast) ---
        clean_text = text.strip()
        
        if type == 'title_selector':
            if len(clean_text) < 10: # Titles are rarely super short
                print(f"      ‚ö†Ô∏è Rejected: Title too short (<10 chars)")
                return False
                
        elif type == 'content_selector':
            if len(clean_text) < 50: # Content must be substantial
                print(f"      ‚ö†Ô∏è Rejected: Content too short (<50 chars)")
                return False
        
        # --- 2. AI Verification ---
        try:
            client = AsyncOpenAI(
                api_key=settings.deepseek_api_key,
                base_url="https://api.deepseek.com"
            )
            
            if type == 'title_selector':
                system_prompt = """You are a Quality Assurance bot for a News Scraper.
Is this text a valid NEWS ARTICLE HEADLINE?
YES: "Global markets rally as inflation drops", "Regeringen varsler nye reformer"
NO: "Menu", "Seneste nyt", "Mest l√¶ste", "Forside", "Log ind", "Abonnement"
Return ONLY JSON: {\"is_valid\": true/false}"""
            
            else: # content_selector
                system_prompt = """You are a Quality Assurance bot for a News Scraper.
Is this text valid ARTICLE CONTENT (body text, lead paragraph, or paywall teaser)?
YES: Narrative text, sentences, paragraphs. "Statsministeren udtalte i g√•r..."
YES (Paywall): "Det var en m√∏rk aften... [Log ind for at l√¶se mere]"
NO: Lists of links ("L√¶s ogs√•: ..."), Navigation menus, Cookie banners, Footer text, Metadata only.
Return ONLY JSON: {\"is_valid\": true/false}"""

            user_prompt = f"Analyze:\n{clean_text[:500]}"

            response = await client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=50
            )
            
            content = response.choices[0].message.content.strip().replace('```json', '').replace('```', '')
            result = json.loads(content)
            
            if not result or not result.get('is_valid'):
                print(f"      ‚ö†Ô∏è AI Rejected: Looks like noise/list")
                return False
                
            return True

        except Exception as e:
            print(f"      ‚ö†Ô∏è Verification failed (failing open): {e}")
            return True

    async def verify_search_pattern(self, pattern: str, domain: str, homepage_html: str = None) -> Optional[str]:
        """
        Verify that a search pattern actually works by testing it.
        Performs checks for 200 OK and Soft 404s (redirect to homepage).

        Args:
            pattern: The search URL pattern (e.g., "https://domain.com/search?q={keyword}")
            domain: The domain name
            homepage_html: Optional HTML of the homepage for comparison (Soft 404 check)

        Returns:
            Verified pattern if it works, None if it fails
        """
        if not pattern or '{keyword}' not in pattern:
            return None

        # Test with a simple keyword
        test_url = pattern.replace('{keyword}', 'test')

        try:
            async with httpx.AsyncClient(timeout=10, follow_redirects=True) as client:
                headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"}
                response = await client.get(test_url, headers=headers)

                if response.status_code == 200:
                    # --- Check 1: Did it redirect to homepage? ---
                    final_url = str(response.url).rstrip('/')
                    homepage_url_https = f"https://{domain}".rstrip('/')
                    homepage_url_http = f"http://{domain}".rstrip('/')
                    
                    if final_url == homepage_url_https or final_url == homepage_url_http:
                        print(f"      ‚ö†Ô∏è Rejected: Redirected to homepage (Soft 404): {pattern}")
                        return None

                    # --- Check 2: Is content identical to homepage? (Soft 404 with same URL) ---
                    if homepage_html:
                        # Simple heuristic: Compare Page Titles
                        try:
                            # Parse only title to be fast
                            if '<title>' in response.text and '<title>' in homepage_html:
                                res_title_start = response.text.find('<title>') + 7
                                res_title_end = response.text.find('</title>', res_title_start)
                                res_title = response.text[res_title_start:res_title_end].strip()

                                home_title_start = homepage_html.find('<title>') + 7
                                home_title_end = homepage_html.find('</title>', home_title_start)
                                home_title = homepage_html[home_title_start:home_title_end].strip()

                                if res_title and res_title == home_title:
                                    print(f"      ‚ö†Ô∏è Rejected: Page title identical to Homepage (Soft 404): {pattern}")
                                    return None
                        except Exception:
                            pass # Fallback if parsing fails

                    print(f"      ‚úÖ Search pattern verified: {pattern}")
                    return pattern
                else:
                    print(f"      ‚ö†Ô∏è Search pattern returned {response.status_code}: {pattern}")
                    return None

        except Exception as e:
            print(f"      ‚ö†Ô∏è Search pattern test failed: {e}")
            return None

    async def try_common_search_patterns(self, domain: str, root_url: str, homepage_html: str = None) -> Optional[str]:
        """
        Try common search URL patterns as fallback.

        Args:
            domain: The domain name
            root_url: The root URL
            homepage_html: Optional homepage HTML for verification

        Returns:
            First working pattern or None
        """
        # Common patterns (Danish and English variants)
        patterns = [
            f"{root_url}/soeg?q={{keyword}}",      # Danish URL-safe
            f"{root_url}/s√∏g?q={{keyword}}",       # Danish with √∏
            f"{root_url}/search?q={{keyword}}",    # English
            f"{root_url}/sog?query={{keyword}}",   # Version2 style
            f"{root_url}/?s={{keyword}}",          # WordPress style
        ]

        print(f"   üîç Trying common search patterns for {domain}...")

        for pattern in patterns:
            verified = await self.verify_search_pattern(pattern, domain, homepage_html)
            if verified:
                return verified

        return None

    async def analyze_source_structure(
        self,
        article_html: str,
        homepage_html: str,
        article_url: str,
        root_url: str
    ) -> Dict[str, Optional[str]]:
        """
        Analyze source structure using Standard Selectors + AI Verification.
        """
        validated_selectors = {}
        soup = BeautifulSoup(article_html, 'lxml')
        validation_count = 0

        # === Step 1: Detect Search Pattern (Homepage Analysis via AI) ===
        verified_pattern = None

        try:
            print(f"   ü§ñ Detecting search pattern on homepage via AI...")
            client = AsyncOpenAI(api_key=settings.deepseek_api_key, base_url="https://api.deepseek.com")

            search_prompt = """Analyze this Homepage HTML and find the SEARCH URL pattern.
Look for <form action=\"...\"> or <input name=\"q\">
Return ONLY JSON: {\"search_url_pattern\": \"https://domain.com/search?q={keyword}\"} OR null."""

            response = await client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": search_prompt},
                    {"role": "user", "content": homepage_html[:20000]}  # Increased from 8000 to 20000
                ],
                temperature=0.1, max_tokens=100
            )

            content = response.choices[0].message.content.strip().replace('```json', '').replace('```', '')
            search_res = json.loads(content)
            pattern = search_res.get('search_url_pattern') if search_res else None

            if pattern and '{keyword}' in pattern:
                print(f"   üîç AI suggested: {pattern}")
                # Verify the pattern actually works
                from urllib.parse import urlparse
                parsed = urlparse(root_url)
                domain = parsed.netloc.lower()
                if domain.startswith('www.'):
                    domain = domain[4:]

                verified_pattern = await self.verify_search_pattern(pattern, domain, homepage_html)

        except Exception as e:
            print(f"   ‚ùå AI search pattern detection failed: {e}")

        # If AI pattern didn't work, try common patterns
        if not verified_pattern:
            print(f"   üîÑ AI pattern failed/missing, trying common patterns...")
            from urllib.parse import urlparse
            parsed = urlparse(root_url)
            domain = parsed.netloc.lower()
            if domain.startswith('www.'):
                domain = domain[4:]

            verified_pattern = await self.try_common_search_patterns(domain, root_url, homepage_html)

        # Store result
        if verified_pattern:
            validated_selectors['search_url_pattern'] = verified_pattern
            print(f"   ‚úÖ search_url_pattern: {verified_pattern}")
            validation_count += 1
        else:
            validated_selectors['search_url_pattern'] = None
            print(f"   ‚ö†Ô∏è search_url_pattern: Not found")


        # === Step 2: Detect Selectors (Iterate Generics + AI Verification) ===
        
        for key in ['title_selector', 'content_selector', 'date_selector']:
            validated_selectors[key] = None
            print(f"   üîç Testing candidates for {key}...")
            
            candidates = GENERIC_SELECTORS_MAP.get(key, [])
            
            for selector in candidates:
                try:
                    element = soup.select_one(selector)
                    if not element:
                        continue
                        
                    # Extract Text
                    if element.name == 'meta':
                        text = element.get('content', '')
                    elif key == 'date_selector' and element.has_attr('datetime'):
                        text = element.get('datetime')
                    else:
                        text = element.get_text(strip=True)
                    
                    if not text:
                        continue

                    # === VERIFICATION ===
                    is_valid = False
                    
                    # DATE: Use Heuristic
                    if key == 'date_selector':
                        if re.search(r'202[0-9]', text):
                            is_valid = True
                        else:
                            print(f"      ‚ö†Ô∏è Rejected Date: '{text}' (no year found)")
                    
                    # TITLE & CONTENT: Use AI Judge
                    else:
                        is_valid = await self.verify_content_quality(text, key)

                    if is_valid:
                        validated_selectors[key] = selector
                        validation_count += 1
                        print(f"   ‚úÖ Verified: {selector}")
                        preview = text[:1000].replace('\n', ' ')
                        print(f"      üëÄ Preview: \"{preview}...\"")
                        break # Stop at first valid selector
                
                except Exception as e:
                    continue
            
            if not validated_selectors[key]:
                print(f"   ‚ùå No valid {key} found in generic list.")


        # Determine confidence
        if validation_count >= 3:
            confidence = "high"
        elif validation_count >= 2:
            confidence = "medium"
        else:
            confidence = "low"

        validated_selectors['confidence'] = confidence
        print(f"üéØ Analysis complete. Confidence: {confidence}")
        
        return validated_selectors